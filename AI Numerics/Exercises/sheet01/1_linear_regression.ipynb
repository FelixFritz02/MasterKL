{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "import numpy as np\n",
    "from utils import make_data, evaluate_regression_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Gradient descent\n",
    "In this exercise we will apply it to linear regression problem. We can use the `make_data` function defined in `utils.py` to generate some data. We will use the same function throughout the session.\n",
    "\n",
    "The generated data (and the regression using the $\\theta$ that was employed  to create them) is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, theta_true = make_data(D=1, T=200)\n",
    "x_plot, y_plot = evaluate_regression_plot(theta=theta_true)\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(x_plot, y_plot, c='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33420d2",
   "metadata": {},
   "source": [
    "# Solving the problem with the analytical solution\n",
    "Recall, we **do not need** to use gradient descent to solve the problem, as it can be solved in closed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd638d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_a = 0 # TODO: complete here\n",
    "x_plot, y_plot = evaluate_regression_plot(theta=theta_a)\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(x_plot, y_plot, c='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4089bf1",
   "metadata": {},
   "source": [
    "## Solving the problem with gradient descent\n",
    "However, we will solve the problem with gradient descent. In order to do so, we need to:\n",
    "- Define a loss function (that we want to minimize)\n",
    "- Define the gradient of the loss function (that we use to modify the parameters)\n",
    "- Pick initial point and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "As loss, we can use:\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{T} \\Vert X\\theta-y\\Vert^2_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_val(X, y, theta):\n",
    "    return 0 # TODO: complete here\n",
    "\n",
    "def grad(X, y, theta):\n",
    "    return 0 # TODO: complete here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We start with a random initialization of $\\theta$ and an arbitrary small value of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = (np.random.rand(X.shape[1], 1)-0.5)*5\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Step\n",
    "\n",
    "At this point, it is useful to visualize the space of possible combinations of values for $\\theta$. Since the regression is 1 dimensional, we can easily plot it in a 2D plot (one axis for the intercept and one for the coefficient).\n",
    "\n",
    "We can color the space with the value of the objective function obtained using this specific $\\theta$. Let us also indicate the negative gradient at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 50\n",
    "xx = np.linspace(-10.0, 10.0, res)\n",
    "\n",
    "z = np.zeros([res,res])\n",
    "for i1,x1 in enumerate(xx):\n",
    "    for i2,x2 in enumerate(xx):\n",
    "        z[i2,i1] = obj_val(X,y,np.array([[x1],[x2]]))\n",
    "\n",
    "grad_init = grad(X,y,theta_init)\n",
    "plt.contourf(xx, xx, z)\n",
    "plt.scatter(theta_init[0], theta_init[1])\n",
    "plt.quiver(theta_init[0],theta_init[1],-grad_init[0],-grad_init[1])\n",
    "plt.xlabel(r\"$\\theta_0$\")\n",
    "plt.ylabel(r\"$\\theta_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "So, in each iteration, we would like to take a step in the direction of the negative gradient, using $\\alpha$ as scaling factor.\n",
    "\n",
    "Practically speaking, \"better\" parameters $\\theta^{\\text{new}}$ can be obtained as:\n",
    "$$\\theta^{\\text{new}} = \\alpha \\cdot-\\nabla_\\theta \\mathcal{L}(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_next = 0 # TODO: complete here\n",
    "\n",
    "plt.contourf(xx, xx, z)\n",
    "plt.scatter(theta_init[0], theta_init[1])\n",
    "plt.scatter(theta_next[0], theta_next[1])\n",
    "plt.quiver(theta_init[0],theta_init[1],-grad_init[0],-grad_init[1])\n",
    "plt.xlabel(r\"$\\theta_0$\")\n",
    "plt.ylabel(r\"$\\theta_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Iterative procedure\n",
    "We can then repeat the iteration until convergence (i.e., updates lead to sufficiently small decrease in the objective function). Let us start from the beginning and run a full GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 500\n",
    "\n",
    "\n",
    "# TODO: complete here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740f9f2",
   "metadata": {},
   "source": [
    "Plot the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(xx, xx, z)\n",
    "plt.scatter(thetas[0, :], thetas[1, :])\n",
    "plt.plot(thetas[0, :], thetas[1, :])\n",
    "plt.scatter(theta_true[0], theta_true[1], c='orange')\n",
    "plt.xlabel(r\"$\\theta_0$\")\n",
    "plt.ylabel(r\"$\\theta_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b47ea43",
   "metadata": {},
   "source": [
    "Plot the distance to the optimal value per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9478c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a621820",
   "metadata": {},
   "source": [
    "[optional] Plot an animation of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "frames = np.linspace(1, len(obj_vals), dtype=np.int64)\n",
    "# frames = np.arange(1, len(obj_vals), dtype=np.int64)\n",
    "\n",
    "mosaic = \"\"\"\n",
    "AACC\n",
    "BBCC\n",
    "\"\"\"\n",
    "fig, ax = plt.subplot_mosaic(mosaic, figsize=(12,8))\n",
    "\n",
    "line_objs, = ax[\"A\"].plot(0, obj_vals[0])\n",
    "line_dist, = ax[\"A\"].plot(0, dists[0])\n",
    "\n",
    "ax[\"B\"].scatter(X[:,1], y)\n",
    "line_regr, = ax[\"B\"].plot([],[])\n",
    "\n",
    "ax[\"B\"].set_xlabel(r\"$x$\")\n",
    "ax[\"B\"].set_ylabel(r\"$y$\")\n",
    "\n",
    "\n",
    "\n",
    "ax[\"C\"].contourf(xx, xx, z)\n",
    "ax[\"C\"].scatter(theta_true[0], theta_true[1], c='r')\n",
    "ax[\"C\"].set_xlabel(r\"$\\theta_0$\")\n",
    "ax[\"C\"].set_ylabel(r\"$\\theta_1$\")\n",
    "scatter_theta, = ax[\"C\"].plot([],[], 'ro')\n",
    "line_theta, = ax[\"C\"].plot([],[])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "def update(i):\n",
    "    line_objs.set_data(frames[:i].flatten(), obj_vals[:i].flatten())\n",
    "    ax[\"A\"].set_xlim(0, i+1)\n",
    "    ax[\"A\"].set_ylim(0, 27)\n",
    "    ax[\"A\"].set_xticks(frames[:i].flatten(), frames[:i].flatten())\n",
    "    line_dist.set_data(frames[:i].flatten(), dists[:i].flatten())\n",
    "\n",
    "    x_plot_curr, y_plot_curr = evaluate_regression_plot(theta=thetas[:,[i-1]])\n",
    "\n",
    "    line_regr.set_data(x_plot_curr, y_plot_curr)\n",
    "\n",
    "    scatter_theta.set_data(thetas[0,i-1].flatten(), thetas[1,i-1].flatten())\n",
    "    line_theta.set_data(thetas[0,:i].flatten(), thetas[1,:i].flatten())\n",
    "    return line_objs,line_dist, scatter_theta, line_theta, line_regr\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig, update, frames=len(frames)\n",
    ")\n",
    "plt.close()\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72849f87",
   "metadata": {},
   "source": [
    "The animation can be saved as a video if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fd1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save('GD_linear_regression.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
