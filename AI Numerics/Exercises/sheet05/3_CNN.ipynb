{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations (normalize to [0,1])\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the data\n",
    "data_train_and_val = MNIST(\"mnist_data\", download=True, transform=transform)\n",
    "train_size = int(0.9*len(data_train_and_val))\n",
    "data_train, data_val = random_split(data_train_and_val, [train_size, len(data_train_and_val)- train_size])\n",
    "data_test = MNIST(\"mnist_data_val\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # (28x28) -> (28x28)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # (28x28) -> (28x28)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # (28x28) -> (14x14)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)  # No softmax (handled in loss function)\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_val_loss = np.inf\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train() # Set model in training mode\n",
    "    loss_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "    \n",
    "    loss_train /= len(train_loader) # Average error \n",
    "\n",
    "    model.eval() # Set model in evaluation mode\n",
    "    loss_val = 0\n",
    "    correct, total = 0,0\n",
    "    with torch.no_grad():\n",
    "        for images,labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_val += loss.item()\n",
    "\n",
    "            _, pred = torch.max(outputs,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred==labels).sum().item()\n",
    "    \n",
    "    loss_val /= len(val_loader)\n",
    "    acc_val = 100 * correct/total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {loss_train:.4f}, Val Loss = {loss_val:.4f}, Val Acc = {acc_val:.2f}%\")\n",
    "\n",
    "\n",
    "    # Save best model\n",
    "    if loss_val < best_val_loss:\n",
    "        best_val_loss = loss_val\n",
    "        torch.save(model.state_dict(), \"best_cnn_mnist.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_cnn_mnist.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Get the weights from the first convolutional layer\n",
    "filters = model.conv1.weight.data  # Shape: (32, 1, 3, 3) -> 32 filters of size 3x3\n",
    "\n",
    "# Normalize for visualization\n",
    "filters = (filters - filters.min()) / (filters.max() - filters.min())\n",
    "\n",
    "# Plot filters\n",
    "fig, axes = plt.subplots(4, 8, figsize=(8, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < filters.shape[0]:  # Ensure we don't go out of bounds\n",
    "        ax.imshow(filters[i, 0].cpu().numpy(), cmap=\"gray\")  # Convert to NumPy\n",
    "        ax.axis(\"off\")\n",
    "plt.suptitle(\"Learned Filters from Conv1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def visualize_feature_maps(model, image):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert image to a batch (add batch dimension)\n",
    "    image = image.unsqueeze(0)  # Shape: (1, 1, 28, 28)\n",
    "\n",
    "    # Forward pass through only the first conv layer\n",
    "    with torch.no_grad():\n",
    "        activation = model.conv1(image)  # Shape: (1, 32, 28, 28)\n",
    "\n",
    "    # Normalize activation maps for visualization\n",
    "    activation = activation.squeeze(0).cpu().numpy()  # Remove batch dim -> (32, 28, 28)\n",
    "    activation = (activation - activation.min()) / (activation.max() - activation.min())\n",
    "\n",
    "    # Plot feature maps\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(10, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < activation.shape[0]:  # Ensure we don't exceed available filters\n",
    "            ax.imshow(activation[i], cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "    plt.suptitle(\"Feature Maps from Conv1\")\n",
    "    plt.show()\n",
    "\n",
    "    # Forward pass through only the second conv layer\n",
    "    # Forward pass through the first conv layer, second conv layer, and pooling\n",
    "    with torch.no_grad():\n",
    "        x = model.conv1(image)  # After first conv: Shape (1, 32, 28, 28)\n",
    "        x = torch.relu(x)  # Apply ReLU activation\n",
    "        x = model.conv2(x)  # After second conv: Shape (1, 64, 28, 28)\n",
    "        x = torch.relu(x)  # Apply ReLU activation\n",
    "        x = model.pool(x)  # After max-pooling: Shape (1, 64, 14, 14)\n",
    "\n",
    "    # Normalize activation maps for visualization\n",
    "    activation = x.squeeze(0).cpu().numpy()  # Remove batch dim -> (32, 28, 28)\n",
    "    activation = (activation - activation.min()) / (activation.max() - activation.min())\n",
    "\n",
    "    # Plot feature maps\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(10, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < activation.shape[0]:  # Ensure we don't exceed available filters\n",
    "            ax.imshow(activation[i], cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "    plt.suptitle(\"Feature Maps from Conv2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image from the test dataset\n",
    "sample_image, _ = data_test[1]  # Get image only (ignore label)\n",
    "plt.imshow(sample_image.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"Original Input Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature maps\n",
    "visualize_feature_maps(model, sample_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get the logits (output from the final fully connected layer) for all test data points\n",
    "logits = []\n",
    "labels = []\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        outputs = model(images)  # Get raw logits (before softmax)\n",
    "        logits.append(outputs.cpu().numpy())  # Store logits\n",
    "        labels.append(targets.cpu().numpy())  # Store labels\n",
    "\n",
    "logits = np.concatenate(logits, axis=0)  # Shape: (num_samples, 10)\n",
    "labels = np.concatenate(labels, axis=0)  # Shape: (num_samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction (10 -> 2)\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(logits)\n",
    "\n",
    "# Visualize with scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=3)\n",
    "plt.colorbar(label=\"Digit Class\")\n",
    "plt.title(\"PCA of Logits from Final Layer\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
