{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ac84dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c1b3f",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e0d2fccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 12)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DataLoader import HeartFailureDataset\n",
    "\n",
    "load_data = HeartFailureDataset()\n",
    "X = load_data.get_features()\n",
    "y = load_data.get_targets()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34eafa",
   "metadata": {},
   "source": [
    "We need to scale our data. Without scaling the data, the nerual network does not work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03826ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd6e33",
   "metadata": {},
   "source": [
    "### **Set up Logistic Regression model**\n",
    "We want to set up our logistic regression by using a neural network. We will use one Linear Layer and the sigmoid function to transform our data in a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ee9bb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_dimensions):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features = input_dimensions, out_features = 1)\n",
    "\n",
    "    def forward(self,x): # Define forard function which applies sigmoid function to our output \n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e6e307b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression(input_dimensions=X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe132ff",
   "metadata": {},
   "source": [
    "So far, we will use the *binary cross-entropy-loss* (**BCELoss**). Later on, one can try different other loss functions.  \n",
    "For a standard Logistic Regression without any regularization, we can use *Stochastic Gradient Descent* as an optimization tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d8a1b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.BCELoss() \n",
    "optimizer = optim.SGD(LR_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e802e",
   "metadata": {},
   "source": [
    "### **Training step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "abf606aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "19fc2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.6949\n",
      "Epoch 50: Loss = 0.6386\n",
      "Epoch 100: Loss = 0.5947\n",
      "Epoch 150: Loss = 0.5601\n",
      "Epoch 200: Loss = 0.5325\n",
      "Epoch 250: Loss = 0.5103\n",
      "Epoch 300: Loss = 0.4921\n",
      "Epoch 350: Loss = 0.4770\n",
      "Epoch 400: Loss = 0.4645\n",
      "Epoch 450: Loss = 0.4538\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "# We wish to save the trajectory of weight, bias and loss\n",
    "hist_W = []\n",
    "hist_b = []\n",
    "hist_loss = []\n",
    "\n",
    "#Trainigsloop bleibt auch bei mehrschichtigen Netzen gleich\n",
    "for epoch in range(epochs):\n",
    "    pred = LR_model(X_tensor)                       # Forward pass\n",
    "    loss = loss_fun(pred, Y_tensor)                 # Compute loss\n",
    "\n",
    "    optimizer.zero_grad()   # Gradienten auf Null setzen\n",
    "    loss.backward()         # Berechne den Gradienten\n",
    "    optimizer.step()        # Optimierungsschritt nach Gradient Descent \n",
    "\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
